{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe8704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core import Document, PropertyGraphIndex, Settings\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c5a99",
   "metadata": {},
   "source": [
    "### Ollama and Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_csv(filepath):\n",
    "    \"\"\"Load CSV data\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def safe_float(value, default=0.0):\n",
    "    \"\"\"Convert to float safely\"\"\"\n",
    "    try:\n",
    "        return float(str(value).strip())\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def create_trade_documents(export_import_data, export_data, import_data):\n",
    "    \"\"\"Create LlamaIndex documents from CSV data\"\"\"\n",
    "    documents = []\n",
    "    doc_id = 0\n",
    "    \n",
    "    # Process export-import summary\n",
    "    partner_summary = {}\n",
    "    for record in export_import_data:\n",
    "        partner = record.get('Partner Name', '').strip()\n",
    "        if not partner or partner == 'India':\n",
    "            continue\n",
    "        \n",
    "        year = record.get('Year', '2022')\n",
    "        trade_balance = safe_float(record.get('Trade Balance (US$ Thousand)', 0))\n",
    "        export_value = safe_float(record.get('Export (US$ Thousand)', 0))\n",
    "        import_value = safe_float(record.get('Import (US$ Thousand)', 0))\n",
    "        export_share = safe_float(record.get('Export Share in Total Products (%)', 0))\n",
    "        import_share = safe_float(record.get('Import Share in Total Products (%)', 0))\n",
    "        num_export_hs6 = safe_float(record.get('No Of exported HS6 digit Products', 0))\n",
    "        num_import_hs6 = safe_float(record.get('No Of imported HS6 digit Products', 0))\n",
    "        \n",
    "        trade_status = \"surplus\" if trade_balance > 0 else \"deficit\" if trade_balance < 0 else \"balanced\"\n",
    "        \n",
    "        text = f\"\"\"India has a trade relationship with {partner} in {year}. \n",
    "        India exports ${export_value} thousand to {partner} ({export_share}% share, {num_export_hs6} products).\n",
    "        India imports ${import_value} thousand from {partner} ({import_share}% share, {num_import_hs6} products).\n",
    "        Trade balance is ${trade_balance} thousand ({trade_status}).\"\"\"\n",
    "        \n",
    "        documents.append(Document(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                'type': 'trade_summary',\n",
    "                'reporter': 'India',\n",
    "                'partner': partner,\n",
    "                'year': year,\n",
    "                'trade_balance': trade_balance,\n",
    "                'export_value': export_value,\n",
    "                'import_value': import_value\n",
    "            },\n",
    "            doc_id=f\"summary_{doc_id}\"\n",
    "        ))\n",
    "        doc_id += 1\n",
    "        partner_summary[partner] = True\n",
    "    \n",
    "    # Process export products\n",
    "    export_by_partner = defaultdict(list)\n",
    "    for record in export_data:\n",
    "        partner = record.get('Partner Name', '').strip()\n",
    "        if not partner or partner == 'India':\n",
    "            continue\n",
    "        \n",
    "        product = record.get('Product Group', '').strip()\n",
    "        value = safe_float(record.get('Export (US$ Thousand)', 0))\n",
    "        share = safe_float(record.get('Export Product Share (%)', 0))\n",
    "        year = record.get('Year', '2022')\n",
    "        \n",
    "        export_by_partner[partner].append({\n",
    "            'product': product,\n",
    "            'value': value,\n",
    "            'share': share,\n",
    "            'year': year\n",
    "        })\n",
    "    \n",
    "    for partner, products in export_by_partner.items():\n",
    "        product_list = \", \".join([f\"{p['product']} (${p['value']} thousand, {p['share']}% share)\" for p in products[:5]])\n",
    "        text = f\"India exports to {partner}: {product_list}.\"\n",
    "        \n",
    "        documents.append(Document(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                'type': 'export_detail',\n",
    "                'reporter': 'India',\n",
    "                'partner': partner,\n",
    "                'products': [p['product'] for p in products],\n",
    "                'total_value': sum(p['value'] for p in products)\n",
    "            },\n",
    "            doc_id=f\"export_{doc_id}\"\n",
    "        ))\n",
    "        doc_id += 1\n",
    "    \n",
    "    # Process import products\n",
    "    import_by_partner = defaultdict(list)\n",
    "    for record in import_data:\n",
    "        partner = record.get('Partner Name', '').strip()\n",
    "        if not partner or partner == 'India':\n",
    "            continue\n",
    "        \n",
    "        product = record.get('Product Group', '').strip()\n",
    "        value = safe_float(record.get('Import (US$ Thousand)', 0))\n",
    "        share = safe_float(record.get('Import Product Share (%)', 0))\n",
    "        year = record.get('Year', '2022')\n",
    "        \n",
    "        import_by_partner[partner].append({\n",
    "            'product': product,\n",
    "            'value': value,\n",
    "            'share': share,\n",
    "            'year': year\n",
    "        })\n",
    "    \n",
    "    for partner, products in import_by_partner.items():\n",
    "        product_list = \", \".join([f\"{p['product']} (${p['value']} thousand, {p['share']}% share)\" for p in products[:5]])\n",
    "        text = f\"India imports from {partner}: {product_list}.\"\n",
    "        \n",
    "        documents.append(Document(\n",
    "            text=text,\n",
    "            metadata={\n",
    "                'type': 'import_detail',\n",
    "                'reporter': 'India',\n",
    "                'partner': partner,\n",
    "                'products': [p['product'] for p in products],\n",
    "                'total_value': sum(p['value'] for p in products)\n",
    "            },\n",
    "            doc_id=f\"import_{doc_id}\"\n",
    "        ))\n",
    "        doc_id += 1\n",
    "    \n",
    "    print(f\"‚úì Created {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "def setup_llamaindex_with_neo4j(neo4j_url, neo4j_username, neo4j_password, \n",
    "                                  ollama_base_url=\"http://localhost:11434\",\n",
    "                                  ollama_model=\"llama3.2-vision\"):\n",
    "    \"\"\"Setup LlamaIndex with Neo4j, local Ollama LLM, and HuggingFace embeddings\"\"\"\n",
    " \n",
    "    \n",
    "    # Configure local Ollama LLM\n",
    "    llm = Ollama(model=ollama_model, base_url=ollama_base_url, request_timeout=120.0)\n",
    "    print(f\"‚úì Using local Ollama LLM: {ollama_model}\")\n",
    "    \n",
    "    # Use HuggingFace embeddings (free, local, no rate limits)\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    print(\"‚úì Using HuggingFace embeddings (local, no rate limits)\")\n",
    "    \n",
    "    # Set global settings\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.chunk_size = 512\n",
    "    \n",
    "    # Setup Neo4j graph store\n",
    "    graph_store = Neo4jPropertyGraphStore(\n",
    "        username=neo4j_username,\n",
    "        password=neo4j_password,\n",
    "        url=neo4j_url,\n",
    "        database=\"neo4j\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì LlamaIndex configured with Ollama and Neo4j\")\n",
    "    return llm, embed_model, graph_store\n",
    "\n",
    "\n",
    "def load_index(persist_dir=\"./storage\", graph_store=None, llm=None, embed_model=None,documents=None):\n",
    "    \"\"\"Load index from disk\"\"\"\n",
    "  \n",
    "    \n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"index found at {persist_dir}\")\n",
    "        # return None\n",
    "    \n",
    "        try:\n",
    "            # Load storage context\n",
    "            storage_context = StorageContext.from_defaults(\n",
    "                persist_dir=persist_dir,\n",
    "                property_graph_store=graph_store\n",
    "            )\n",
    "            \n",
    "            # Load index\n",
    "            index = load_index_from_storage(\n",
    "                storage_context,\n",
    "                property_graph_store=graph_store,\n",
    "                llm=llm,\n",
    "                embed_model=embed_model\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úì Index loaded from {persist_dir}\")\n",
    "            return index\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading index: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    \"\"\"Create PropertyGraphIndex from documents\"\"\"\n",
    "   \n",
    "    \n",
    "    print(\"\\n[Building Property Graph Index]\")\n",
    "    print(f\"Processing {len(documents)} documents...\")\n",
    "    print(\"Using local LLM and embeddings - no API rate limits!\")\n",
    "    print(\"This will take 10-15 minutes...\")\n",
    "    \n",
    "    # Process ONE document at a time\n",
    "    index = None\n",
    "    successful_count = 0\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_num = i + 1\n",
    "        print(f\"\\nüìÑ Processing document {doc_num}/{len(documents)}...\", end=\" \")\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 2\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                if index is None:\n",
    "                    # First document - create new index\n",
    "                    index = PropertyGraphIndex.from_documents(\n",
    "                        [doc],\n",
    "                        property_graph_store=graph_store,\n",
    "                        llm=llm,\n",
    "                        embed_model=embed_model,\n",
    "                        show_progress=False\n",
    "                    )\n",
    "                else:\n",
    "                    # Subsequent documents - add to existing index\n",
    "                    index.insert(doc)\n",
    "                    \n",
    "                successful_count += 1\n",
    "                print(f\"‚úì (Total: {successful_count})\")\n",
    "                \n",
    "                # Small delay to not overwhelm Ollama\n",
    "                if doc_num < len(documents):\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                error_msg = str(e)\n",
    "                \n",
    "                if \"memory\" in error_msg.lower() or \"oom\" in error_msg.lower():\n",
    "                    print(f\"\\n   ‚ùå Memory error: Your Ollama model is too large!\")\n",
    "                    print(f\"   Switch to a smaller model like 'llama3.2' or 'phi3'\")\n",
    "                    raise Exception(\"Out of memory - use smaller model\")\n",
    "                elif retry_count < max_retries:\n",
    "                    print(f\"\\n   ‚ö†Ô∏è Error: {error_msg[:100]}... Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"\\n   ‚ùå Failed after {max_retries} attempts: {error_msg[:100]}\")\n",
    "                    break\n",
    "        \n",
    "        # Show progress every 10 documents\n",
    "        if doc_num % 10 == 0:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Progress: {successful_count}/{doc_num} documents processed successfully\")\n",
    "            print(f\"{'='*50}\")\n",
    "    \n",
    "    if index is None:\n",
    "        raise Exception(\"Failed to create index - all documents failed\")\n",
    "    \n",
    "    print(f\"\\n‚úì Property Graph Index created successfully\")\n",
    "    print(f\"‚úì Successfully processed {successful_count}/{len(documents)} documents\")\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "def query_graph_index(index, question):\n",
    "    \"\"\"Query the property graph index\"\"\"\n",
    "    \n",
    "    # Create query engine with hybrid retrieval (vector + graph)\n",
    "    query_engine = index.as_query_engine(\n",
    "        include_text=True,\n",
    "        response_mode=\"tree_summarize\",\n",
    "        similarity_top_k=5\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(question)\n",
    "    return response\n",
    "\n",
    "def run_sample_queries(index,questions):\n",
    "    \"\"\"Run sample queries\"\"\"\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAMPLE QUERIES - LLAMAINDEX PROPERTY GRAPH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for q in questions:\n",
    "        print(f\"\\nüìä Q: {q}\")\n",
    "        response = query_graph_index(index, q)\n",
    "        print(f\"üí° A: {response}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "def get_graph_statistics(graph_store):\n",
    "    \"\"\"Get statistics from Neo4j graph\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    try:\n",
    "        # Count entities\n",
    "        result = graph_store._driver.execute_query(\n",
    "            \"MATCH (n) RETURN count(n) as count\"\n",
    "        )\n",
    "        stats['total_nodes'] = result.records[0]['count'] if result.records else 0\n",
    "        \n",
    "        # Count relationships\n",
    "        result = graph_store._driver.execute_query(\n",
    "            \"MATCH ()-[r]->() RETURN count(r) as count\"\n",
    "        )\n",
    "        stats['total_relationships'] = result.records[0]['count'] if result.records else 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch stats: {e}\")\n",
    "        stats['total_nodes'] = \"Unknown\"\n",
    "        stats['total_relationships'] = \"Unknown\"\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def print_statistics(stats):\n",
    "    \"\"\"Print graph statistics\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROPERTY GRAPH STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Nodes: {stats['total_nodes']}\")\n",
    "    print(f\"Total Relationships: {stats['total_relationships']}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "def clear_neo4j_graph(graph_store):\n",
    "    \"\"\"Clear Neo4j database\"\"\"\n",
    "    try:\n",
    "        graph_store._driver.execute_query(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"‚úì Neo4j database cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not clear database: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8063d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":     \n",
    "\n",
    "\n",
    "    OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"  \n",
    "    OLLAMA_MODEL = \"llama3.2-vision\"  \n",
    "  \n",
    "    \n",
    "    # Neo4j connection parameters\n",
    "    NEO4J_URL = \"YOUR_NEO4J_URL\" \n",
    "    NEO4J_USERNAME = \"YOUR_NEO4J_USERNAME\"\n",
    "    NEO4J_PASSWORD = \"YOUR_NEO4J_PASSWORD\"\n",
    "\n",
    "    EXPORT_IMPORT_FILE = r\"Trade data\\ex_im_data.csv\"\n",
    "    EXPORT_FILE = r\"Trade data\\export_data.csv\"\n",
    "    IMPORT_FILE = r\"Trade data\\import_data.csv\"\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"\\n[1] Loading CSV data...\")\n",
    "    export_import_data = load_csv(EXPORT_IMPORT_FILE)\n",
    "    export_data = load_csv(EXPORT_FILE)\n",
    "    import_data = load_csv(IMPORT_FILE)\n",
    "    print(f\"‚úì Loaded {len(export_import_data)} summary records\")\n",
    "    print(f\"‚úì Loaded {len(export_data)} export records\")\n",
    "    print(f\"‚úì Loaded {len(import_data)} import records\")\n",
    "    \n",
    "    print(\"\\n[2] Creating documents...\")\n",
    "    documents = create_trade_documents(export_import_data, export_data, import_data)\n",
    "        \n",
    "    # Step 3: Setup LlamaIndex with Neo4j and Ollama\n",
    "    print(\"\\n[3] Setting up LlamaIndex with Neo4j and local Ollama...\")\n",
    "    llm, embed_model, graph_store = setup_llamaindex_with_neo4j(\n",
    "        NEO4J_URL, NEO4J_USERNAME, NEO4J_PASSWORD,\n",
    "        OLLAMA_BASE_URL, OLLAMA_MODEL\n",
    "    )\n",
    "\n",
    "    if graph_store is None:\n",
    "        print(\"‚ùå Failed to setup LlamaIndex\")\n",
    "        print(\"\\nTo setup:\")\n",
    "        print(\"1. Install and start Neo4j\")\n",
    "        print(\"2. Install Ollama: https://ollama.ai\")\n",
    "        print(\"3. Pull model: ollama pull llama3.2-vision\")\n",
    "        print(\"4. Update NEO4J_PASSWORD in code\")\n",
    "        exit(1)\n",
    "\n",
    "    # Step 4: Clear previous data\n",
    "    print(\"\\n[4] Clearing previous Neo4j data...\")\n",
    "    clear_neo4j_graph(graph_store)\n",
    "\n",
    "    # Step 5: Build Property Graph Index\n",
    "    print(\"\\n[5] Building Property Graph Index...\")\n",
    "    index = load_index(documents =documents,graph_store=graph_store,llm=llm,embed_model=embed_model)\n",
    "    # Step 4: Get statistics\n",
    "    print(\"\\n[4] Getting graph statistics...\")\n",
    "    stats = get_graph_statistics(graph_store)\n",
    "    print_statistics(stats)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What does India export to United Arab Emirates?. Enlist down the product too!\",\n",
    "]\n",
    "for q in questions:\n",
    "    nest_asyncio.apply()\n",
    "    run_sample_queries(index,[q])\n",
    "    print(\"Done!!!!\\n\")\n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
